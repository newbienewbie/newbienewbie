---
layout: post
title: 机器学习 梯度下降法
date: 2018-06-12 08:55:23
tags:
- 炼丹
- 机器学习
- 神经网络
- 梯度下降法
categories:
- 大道
- 炼丹
- 机器学习
---

在[上一篇中机器学习的笔记中](/blog/2018/04/10/Boulevard/Alchemy/machine-learning/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%20-%20%E4%B8%96%E7%95%8C%E8%A7%82/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%20%E4%B8%96%E7%95%8C%E8%A7%82%203/)，我们讨论了朴素的梯度下降法的思想，本篇给出一个通用的理论。

假设有一个性能函数`$F$`(可以类比做之前博文中提到的成本函数)，该性能函数接受多个维度的自变量`$x^{1},x^{2},x^{3}, ... $`，不妨记为`$\bold x$`，故该函数可以表示为`$F(\bold x)$`。

为了令性能函数`$F$`取得最低值，可以利用计算机反复迭代：
```math
%% KaTex
\bold x_{k+1} = \bold x_{k} + \Delta \bold x
```
其中，`$\Delta \bold x = \alpha \bold p$`，这里的`$\bold p$`代表某个方向向量——指向某个搜索方向的向量。要令`$F(\bold x)$`取得极小值，就要令每次迭代计算后的`$F(\bold x_{k+1})$`比`$F(\bold x_{k})$`小。<!-- more -->

将上述迭代式代入`$F(\bold x)$`函数得到`$F(\bold x_{k+1}) = F(\bold x_{k} + \Delta x_{k})$`，根据多元函数形式的泰勒公式，对其右侧进行一阶展开，即有:
```math
%% KaTex
F(\bold x_{k+1}) \approx F(\bold x_{k}) + \underbrace{\nabla F(\bold x)^{T} \vert _{\bold x = \bold x_{k}}}_{\text{记作} \bold g^{T}_{k}} \cdot \underbrace{\Delta \bold x_{k}}_{\text{=} \alpha_{k} \bold p_{k}}
```
故
```math
%% KaTex
F(\bold x_{k+1}) = F(\bold x_{k}) + \alpha_{k} \bold g^{T}_{k} \bold p_{k} 
```
其中，`$\alpha_{k}$`表示朝当前搜素方向进行迭代的步进速率，`$\bold g^{T}_{k}$`表示当前的梯度向量，`$\bold p_{k}$`表示当前的搜素方向。

根据上式，要让`$F(\bold x_{k+1})$`比`$F(\bold x_{k})$`更小，只需要令右式为负值即可。而当`$\bold p$`和梯度向量方向相反时，二者的点乘取得最小值(绝对值最大，符号为负)。故在每一步迭代时，取`$\bold p_{k} = - \bold g_{k}$`即意味着沿着最快下降方向进行搜素。所以，迭代式可以写成：
```math
%% KaTex
\bold x_{k+1} = \bold x_{k} - \alpha_{k} \bold g_{k} 
```
也即每次沿着梯度的反方向进行迭代搜素。
