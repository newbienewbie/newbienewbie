---
layout: post
title: 机器学习 世界观 成本函数与另一种有监督的学习算法
date: 2018-04-10 22:55:34
tags:
- 炼丹
- 机器学习
- 神经网络
- Adaline
categories:
- 大道
- 炼丹
- 机器学习
---

对于训练集`$\bold X$`中第`$i$`个`Sample` `$\bold x^{(i)}$`，记其目标值为`$y^{(i)}$`，预测值为`$\hat y^{(i)}$`。对于训练集中的样品，训练后的最好效果是，各预测值与相应的实际值完全相同，但实际情况必然是互有大小。为了衡量这组值的误差大小，可以用 *平方和* (`SSE`) 来表示这种误差：
```math
%% KaTex
J(w) = \frac{1}{2} \displaystyle \sum_i{\Big(y^{(i)} -\hat y^{(i)} \Big)}^{2}
```
要让训练的效果最好，其实就是要让此`SSE`取得最小值。故而，我们将这个`$J(w)$`称之为 *目标函数* (`objective function`)，目标函数通常是 *成本函数* (`cost function`)，正如这里的`SSE`，训练就是要让成本函数取得最低值。

## 梯度下降法的朴素思想

那么，如何才让成本函数取得最低值呢？

根据泰勒公式，一个函数`$f(x)$`可以利用在`$x=x_0$`展开近似：
```math
%% KaTex
f(x) = f(x_0)+f'(x_0)*(x-x_0)
```
不妨假设`$f(x)$`在`$x=x_0$`上附加一个增量`$\delta x$`(即当点`$x$`位于`$x_0$`的右侧) <!--more-->
* 如果导数`$f'(x_0)$`为正，则`$f(x)$`会比`$f(x_0)$`大，即随着`$x$`变大，函数值会变大
* 如果导数`$f'(x_0)$`为负，则`$f(x)$`会比`$f(x_0)$`小，即随着`$x$`变大，函数值会变小

类似的，当`$x$`位于`$x_0$`左侧:
* 如果导数`$f'(x_0)$`为正数，则`$f(x)$`会比`$f(x_0)$`小，即随着`$x$`变小，函数值会变小。
* 如果导数`$f'(x_0)$`为负数，则`$f(x)$`会比`$f(x_0)$`大，即随着`$x$`变小，函数值会变大。

综合以上情况，我们可以得出非常符合直觉的结论：
* 如果`$f'(x)$`为正，则函数`$f(x)$`的值会随着`$x$`的变大而变大，随着`$x$`的变小而变小
* 如果`$f'(x)$`为负，则函数`$f(x)$`的值会随着`$x$`的变大而变小，随着`$x$`的变小而变大

于是为了找到更小的函数值，我们可以采取这样一种迭代方法：

1. 取`$x=x_0$`为轴点，计算轴点的导数值`$f'(x=x_0)$`
2. 如果`$f'(x_0)$`为负，则说明往右方向函数在减小，为了找到更小的点，需要增大`$x$`，可以将轴点设置为`$x_0$`右侧的一个点(即`$x=x_0 + \delta x$`)，然后继续下一轮迭代。
3. 如果`$f'(x_0)$`为正，则说明往右方向函数在增大，为了找到更小的点，需要调小`$x$`，可以将轴点设置为`$x_0$`左侧的一个点(即`$x=x_0 - \delta x$`)，然后继续下一轮迭代。

注意到
1. `$f'(x_0)$`的正负和`$\delta x$`之前的符号具有相反性
2. 直观上，`$f'(x_0)$`的绝对值越大，表示曲线越陡，可能离`$f'(x)=0$`的极小点越远，需要调整`$\delta x$`的幅度越大

所以，可以把上述迭代规则统一表示为`$x = x_0 - \eta f'(x_0) $`。

类似地，对于多元函数`$J(\bold w)$`，我们可以通过逐步调整`$\bold w$`，也即进行`$\bold w := \bold w + \Delta \bold w$`迭代，最终让`$J(w)$`取得最小(极小)值。这里:
```math
%% KaTex
\Delta \bold w = - \eta \nabla J(\bold w)
```
即沿着梯度方向，不断调整影响成本函数的权重变量，直至成本函数取得极小值。当然，这种方法找到的并不一定式最小点，很有可能是个极小点。不过机器学习作为炼丹玄学，我们大可以碰碰运气——梦想还是要有的，万一实现了呢？


## `SSE`的偏导函数公式推导

梯度下降法需要对偏导函数进行求值，对于采用`SSE`法定义的成本函数，我们可以做一点微积分来化简`SSE`的`$J(\bold w)$`偏导函数：
```math
%% KaTex
\frac{\partial {J}}{\partial {w_j}} = \frac{\partial }{\partial {w_j}} \frac{1}{2} \displaystyle \sum_i{ \Big(y^{(i)} -\hat y^{(i)} \Big)^{2}}
```
如果取`$ \phi(z) = z$`，即选取纯线性函数作为传输函数，显然，上述等式右侧等于：
```math
%% KaTex
\frac{1}{2} \displaystyle \sum_i{2 \Big( y^{(i)} -\hat y^{(i)} \Big)  \Big( \frac{\partial }{\partial {w_j}} \big( y^{(i)} -\hat y^{(i)} \big)  \Big)  = \displaystyle \sum_i{ \Big( y^{(i)} -\hat y^{(i)} \Big) } \Big( \underbrace{ \frac{\partial }{\partial {w_j}} \big( y^{(i)} -\hat y^{(i)} \big) }_{\text{A}} \Big) }
```
对于纯线性传输函数，训练集中的任意一个样品`$i$`，都有`$\hat y^{(i)}=\displaystyle \sum_i{w_j^{(i)}x_j^{(i)} +b^{(i)} }$`，不过偏置`$b^{(i)}$`可以通过平移消除，所以为了简便起见，暂时略去。据此展开上式中的`$A$`部分：
```math
%% KaTex
A = \frac{\partial }{\partial {w_j}} \Big( y^{(i)} -\hat y^{(i)} \Big) =  \frac{\partial }{\partial {w_j}} \Big( y^{(i)} - \displaystyle \sum_i{\big( w_j^{(i)} x_j^{(i)} \big)}  \Big) 
```
由于对任意`$w_j$`而言，目标值`$y^{(i)}$`都是既定常量，故：
```math
%% KaTex
A =  \frac{\partial }{\partial {w_j}} \Big(  - \displaystyle \sum_i{\big( w_j^{(i)} x_j^{(i)} \big)}  \Big) =- x_j^{(i)}
```
综上，得到`$J(w)$`的偏导函数求解公式：
```math
%% KaTex
\color{white} \frac{\partial {J}}{\partial {w_j}} = \color{orange} - \displaystyle \sum_i{ \Big( y^{(i)} -\hat y^{(i)} \Big)  x_j^{(i)} }
```
上式右部实际上可以视作为向量的内积，记误差向量为`$\bold e = \bold y - \hat \bold y= [ (y^{(1)}-\hat y^{(1)}),  (y^{(2)}-\hat y^{(2)}) , ... ,  (y^{(n)}-\hat y^{(n)}) ]^{T}$`于是有：
```math
%% KaTex
\frac{\partial {J}}{\partial {w_j}} = - \underbrace{ [x^{(1)}_{j} , x^{(2)}_{j} , ... ,x^{(n)}_{j} ] }_\text{B} \cdot \bold e 
```
上式的`$B$`部分表示训练集中的第`$j$`个输入所构成的向量(行向量)——也就是训练集`$X$`中的第`$j$`列的转置，不妨记作`$x^{T}_j$`。故有`$\frac{\partial{J}}{\partial{w_j}}$`等于第`$j$`个特性输入样品向量与误差向量的内积。即：
```math
%% KaTex
\frac{\partial {J}}{\partial {w_j}} = - \bold x^{T}_j \cdot \bold e 
```
这样从另一个侧面说明，针对每种特性的输入，偏导函数的值都是一个常量。对于`$\bold w = [w_1, w_2, ... , w_j , ..., w_n]$`，分别计算`$\frac{\partial {J}}{\partial {w_j}}$`，写成列向量的形式，则有：
```
%% KaTex
\nabla J = \begin{bmatrix} \frac{\partial {J}}{\partial {w_1}} \\ \frac{\partial {J}}{\partial {w_2}} \\ ... \\ \frac{\partial {J}}{\partial {w_n}}  \end{bmatrix} = - \begin{bmatrix} \bold x^{T}_1 \cdot \bold e \\ \bold x^{T}_2 \cdot \bold e  \\ ... \\ \bold x^{T}_n \cdot \bold e  \end{bmatrix} = - X^{T} \bold e
```
这意味着，这一系列偏导函数所构成的列向量(梯度向量)，等于训练集`$X$`的转置取负后乘以误差向量`$\bold e$`。

## 算法实现

根据以上分析，训练神经元的过程就是反复进行`$\bold w  := \bold w + \Delta \bold w$`。其中，对于线性传输函数，利用梯度下降法求解`SSE`最小值，有`$ \Delta \bold w  = - \eta * \nabla J = \eta * X^{T} \bold e $`。

据此，很容易写出以下实现：
```python

class AdaptiveLineNeuron:

    # ....
     
    def fit(self,X,y):
        self.weight =[]
        for i in range(self.n_iter):
            y_predicted=self.net_input(X)
            e= y - y_predicted 
            self.weight += self.eta * X.T.dot(e)
    
    def net_input(self,X):
        return np.dot(X , self.weight)
```
