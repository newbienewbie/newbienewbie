---
layout: post
title: 机器学习 世界观 一种有监督学习的算法
date: 2018-04-10 19:03:54
tags:
- 炼丹
- 机器学习
- 神经网络
categories:
- 大道
- 炼丹
- 机器学习
---

训练集可以视作一个矩阵`$\bold X$`，其中每一行都代表一条训练数据向量`$\bold x$`。我们的目标就是要通过训练集反复调整权重向量，得到能够刻画数据内在规律的数学方程。

## 朴素思想

不妨先只考虑单个神经元的情况。对于训练集中的每一组训练数据`$\bold x$`，都通过`$\phi(\bold w \cdot \bold x +b)$`计算出一个预测值`$\hat y$`。 并和真实值`$y$`相比较得到误差`$e=y - \hat y$` 。由于`$\phi(\bold w \cdot \bold x +b)$`是对`$\bold w \cdot \bold x$`简单变换，为了简单起见，我们假设`$\phi$`是正相关函数，这样只需考虑`$\bold w \cdot \bold x$`即可推知`$\phi(z)$`的变化趋势：
1. 如果误差`$e = 0$`，则说明预测值与真实值相同，不需要调整权重向量`$\bold w$`。
2. 如果误差`$e > 0$`，则说明预测值与真实值相比偏小，需要修正`$\bold w$`。我们可以通过调整`$\bold w$`，使之与输入向量`$\bold x$`之间的夹角更小一点，这样计算出来的`$\bold w \cdot \bold x$`值会更大，从而导致预测值`$\phi(z)$`会增大一点。
3. 如果误差`$e < 0$`，则说明预测值与真实值相比偏大，需要修正`$\bold w$`。我们可以通过调整`$\bold w$`，使之与输入向量`$\bold x$`之间的夹角更大一点，这样计算出来的`$\bold w \cdot \bold x$`值会更小，从而导致预测值`$\phi(z)$`会减小一点。

所以，现在的问题在于，能否找到一种简单的策略来调整`$\bold w$`，可以`$\bold w$`让其和`$\bold x$`之间的夹角更大（或者更小）？<!--more-->

数学家根据向量加减法运算的规则，设计出了这样一种算法：
1. 要让`$\bold w$`和`$\bold x$`之间夹角更小，可以令`$\bold w := \bold w + \bold x$`，根据向量加分的平行四边形法则，等式的右边代表向量`$\bold w$`和向量`$\bold x$`的对角线，从而令调整后的`$\bold w$`更加“偏向”于输入向量`$\bold x$`，形成更小的夹角。为了更灵活调节夹角的大小，我们可以取一个系数`$\alpha$`和`$\bold x$`的数乘，得到向量`$\alpha \bold x$`，然后调整`$\bold w$`和其之间的夹角，也即`$\bold w := \bold w + \alpha\bold x$`。
2. 类似的，要让`$\bold w$`和`$\bold x$`之间夹角更大，可以转而去令`$\bold w$`和`$-\bold x$`之间的夹角更小，即可以令`$\bold w :=\bold w - \alpha\bold x$`。

考虑到`$e=y- \hat y$`的取值和上述加法或减法有严格相关性，可以将上述调整`$\bold w$`向量的算法统一表示为：
```math
%% KaTex
\bold w := \bold w +  \eta e \bold x
```
其中，`$\eta$`称之为学习速率，表达了每次调整向量`$\bold w$`和向量`$\bold x$`之间夹角的速度。

事实上，偏置量`$b$`可以视作输入恒为`$1$`时对应的权重，故偏置的调整规则也可以类似地统一表示为：
```math
%% KaTex
b := b + \eta e 
```

## 算法框架

1. 初始化权值向量`$\bold w$`
2. 对于训练集中每一个`$\bold x^{(i)}$`的执行以下操作：
    * 计算当前权重下预测出的输出值`$\hat y$`
    * 计算预测值和真实值之间的误差`$e= y - \hat y$`
    * 调整权重向量`$ \bold w := \bold w + \eta e \bold x $`
    * 调整偏置值 `$ \bold b := \bold b + \eta e $`

## 算法实现

```python
class Perception(object):

    def __init__(self,eta=0.01,n_iter=10):
        self.eta=eta
        self.n_iter=n_iter
        self.weight=None
        self.bias=np.zeros(1)
        self.errors_=[]
    
    def _net_input(self,x):
        return np.dot(x,self.weight) + self.bias
    
    def think(self,x):
        return np.where(self._net_input(x) >= 0.0 ,1, -1)
    
    def train(self,X,y):
        self.weight=np.zeros(X.shape[1])

        for _ in range(self.n_iter):
            errors=0
            for x, target in zip(X,y):
                update=self.eta*(target-self.think(x))
                self.weight +=update * x
                self.bias +=update
                errors +=int(update != 0)
            self.errors_.append(errors)
        return self
```
